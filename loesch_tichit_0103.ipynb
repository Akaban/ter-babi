{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Projet: Facebook BaBi tasks\n",
    "Notebook de la semaine du 27/02\n",
    "Par Thierry Loesch et Bryce TIchit\n",
    "\n",
    "Sur ce notebook nous avons rajouté un parser afin de parser les données et nous avons fait la vectorisation de celles-ci. Un modèle a aussi été créé selon ce qui nous avions défini dans le notebook précédent. Nous avons ainsi pu commencer a faire des tests avec le modèle, et nous avons eu des résultats plutôt satisfaisant sur la première task, environ 70% de précision. Le réseau récurrent utilisé est LSTM sur ce notebook.\n",
    "\n",
    "Reste à perfectionner le modèle et/ou les autres paramètres. Essayer les autres tasks également. Utiliser un Memory Network pour après?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parserBabi(data):\n",
    "    ret=list()\n",
    "    story=list()\n",
    "    \n",
    "    for phrase in data:\n",
    "        phrase = phrase.decode('utf-8').strip()\n",
    "        id_phrase,phrase = int(phrase[0]),phrase[1:]\n",
    "        \n",
    "        if id_phrase == 1: #Nouvelle story\n",
    "            story=list()\n",
    "            \n",
    "        if '\\t' in phrase: #Si tabulation alors il s'agit de la question ainsi que de la réponse\n",
    "            q, a, justif = phrase.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            \n",
    "            data_story = [x for x in story if x] \n",
    "                                             \n",
    "            ret.append((data_story,q,a)) #Nos données d'apprentissages\n",
    "            story.append('')\n",
    "        \n",
    "        else: \n",
    "            #Alors la phrase est tout simplement un des élements de raisonnement et non une question\n",
    "            story.append(tokenize(phrase))\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def readAndParse(f):\n",
    "    data = parserBabi(f.readlines())\n",
    "    return [([substory for substories in story for substory in substories], q , a) for story,q,a in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des données\n",
    "\n",
    "On récupère les fichiers sur internet directement avec la fonction get_file, cette fonction a l'avantage de ne pas tout retelecharger si les données sont déjà sur la machines (dans ~/.keras/datasets). Puis on applique les fonctions du parser afin de récupérer les données dans la structure que l'on veut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import tarfile\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "   print(\"erreur pendant le telechargement\")\n",
    "    \n",
    "tar = tarfile.open(path)\n",
    "challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa19_path-finding_{}.txt'\n",
    "\n",
    "train = readAndParse(tar.extractfile(challenge.format('train')))\n",
    "test = readAndParse(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict(((w,i+1) for i,w in enumerate(vocab)))\n",
    "\n",
    "story_max = max((len(x) for x,_,_ in train+test))\n",
    "question_max = max((len(x) for _,x,_ in train+test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation\n",
    "\n",
    "Comme vu en cours on doit vectoriser nos données (les assimiler a des nombres et les mettre dans des matrices) afin de pouvoir entrainer notre réseau neuronal.\n",
    "\n",
    "La fonction pad_sequences plus bas permet de transformer notre liste de liste en matrice numpy en ajoutant des 0. pour complèter quand il n'y a pas de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mettons tout ça dans des matrices\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(data,word_idx,story_max,question_max):\n",
    "    \n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, question, reponse in train:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in question]\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[reponse]] = 1 \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    \n",
    "    return pad_sequences(X, maxlen=story_max), pad_sequences(Xq, maxlen=question_max), np.array(Y)\n",
    "\n",
    "X,Xq,Y = vectorize(train,word_idx,story_max,question_max)\n",
    "X_test,Xq_test,Y_test = vectorize(test,word_idx,story_max,question_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  7 18 21 20 11  1  6 23 21 20 14  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 18 21 20 11  1  6 23 21\n",
      "  20 14  1  5 23 10 21 20 14  1  8 18 21 20 13  1]\n",
      " [ 0  0  0  7 18 21 20 11  1  6 23 21 20 14  1  5 23 10 21 20 14  1  8 18\n",
      "  21 20 13  1  6 18 21 20 19  1  8 16 21 20 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  8 22 21 20 19  1  8 23 21 20 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8 22 21 20 19  1  8 23\n",
      "  21 20 11  1  7 23 21 20 12  1  5 18 21 20 14  1]\n",
      " [ 0  0  0  0  8 22 21 20 19  1  8 23 21 20 11  1  7 23 21 20 12  1  5 18\n",
      "  21 20 14  1  6 23 21 20 13  1  6 22 21 20 19  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "[[ 0  9 15  7  4]\n",
      " [ 0  9 15  5  4]\n",
      " [ 0  9 15  5  4]\n",
      " [ 2  9 15  5  4]\n",
      " [ 3  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 2  9 15  6  4]\n",
      " [ 3  9 15  5  4]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:10]\n",
    "print Xq[0:10]\n",
    "print Y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle\n",
    "\n",
    "Ensuite nous crééons notre modèle keras, nous avions choisi donc de faire un modèle pour les story et pour les questions afin de pouvoir raisonner sur chacun d'eux différement puis de les combiner en un modèle, ainsi nous avons un seul modèle mais qui traite différement les story des questions. On a un LSTM dans le modèle des story car il faut raisonner d'abord sur les story seules puis les story avec les questions (raisonner sur les questions seule ne veut pas dire grand chose)\n",
    "\n",
    "Schema modèle:\n",
    "\n",
    "                    story_model                              question_model\n",
    "                         |                                           |\n",
    "                      Embedding                                  Embedding\n",
    "                          |                                          |\n",
    "                                                                    /\n",
    "                        LSTM                                       /\n",
    "                          |                                       /\n",
    "                        RepeatVector                            /\n",
    "                                    \\                          /\n",
    "                                      \\_______________________/\n",
    "                                                   |\n",
    "                                                   |\n",
    "                                              Merge(mode=sum)\n",
    "                                                   |\n",
    "                                                 LSTM\n",
    "                                                   |\n",
    "                                                 SoftMax\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Avec un batchsize de 32 un nombre de 60 epochs semble optimal.\n",
    "- Le fait d'avoir utilisé un batchsize petit a amelioré les résultats (32 optimal?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.9705 - acc: 0.1440     \n",
      "Epoch 2/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.7229 - acc: 0.2970     \n",
      "Epoch 3/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.5753 - acc: 0.4070     \n",
      "Epoch 4/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.4939 - acc: 0.4050     \n",
      "Epoch 5/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.4186 - acc: 0.4270     \n",
      "Epoch 6/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.3726 - acc: 0.4190     \n",
      "Epoch 7/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.3177 - acc: 0.4250     \n",
      "Epoch 8/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.3046 - acc: 0.4310     \n",
      "Epoch 9/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.2544 - acc: 0.4300     \n",
      "Epoch 10/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.2060 - acc: 0.4600     \n",
      "Epoch 11/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.1934 - acc: 0.4780     \n",
      "Epoch 12/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.1502 - acc: 0.4990     \n",
      "Epoch 13/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.1232 - acc: 0.5040     \n",
      "Epoch 14/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.1005 - acc: 0.5260     \n",
      "Epoch 15/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.0652 - acc: 0.5120     \n",
      "Epoch 16/60\n",
      "1000/1000 [==============================] - 2s - loss: 1.0239 - acc: 0.5430     \n",
      "Epoch 17/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.9735 - acc: 0.5850     \n",
      "Epoch 18/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.9579 - acc: 0.5770     \n",
      "Epoch 19/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.9085 - acc: 0.6070     \n",
      "Epoch 20/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.8754 - acc: 0.6410     \n",
      "Epoch 21/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.8731 - acc: 0.6150     \n",
      "Epoch 22/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.8451 - acc: 0.6240     \n",
      "Epoch 23/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.8092 - acc: 0.6450     \n",
      "Epoch 24/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.8028 - acc: 0.6460     \n",
      "Epoch 25/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7871 - acc: 0.6740     \n",
      "Epoch 26/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7830 - acc: 0.6540     \n",
      "Epoch 27/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7820 - acc: 0.6640     \n",
      "Epoch 28/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7452 - acc: 0.6690     \n",
      "Epoch 29/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7605 - acc: 0.6710     \n",
      "Epoch 30/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7588 - acc: 0.6620     \n",
      "Epoch 31/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7384 - acc: 0.6730     \n",
      "Epoch 32/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7322 - acc: 0.6710     \n",
      "Epoch 33/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7407 - acc: 0.6670     \n",
      "Epoch 34/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7437 - acc: 0.6620     \n",
      "Epoch 35/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7453 - acc: 0.6650     \n",
      "Epoch 36/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7431 - acc: 0.6750     \n",
      "Epoch 37/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7333 - acc: 0.6720     \n",
      "Epoch 38/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7197 - acc: 0.6780     \n",
      "Epoch 39/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7193 - acc: 0.6740     \n",
      "Epoch 40/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7296 - acc: 0.6850     \n",
      "Epoch 41/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7395 - acc: 0.6740     \n",
      "Epoch 42/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7368 - acc: 0.6650     \n",
      "Epoch 43/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7189 - acc: 0.6760     \n",
      "Epoch 44/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7272 - acc: 0.6760     \n",
      "Epoch 45/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7305 - acc: 0.6800     \n",
      "Epoch 46/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7195 - acc: 0.6780     \n",
      "Epoch 47/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7684 - acc: 0.6670     \n",
      "Epoch 48/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7060 - acc: 0.6820     \n",
      "Epoch 49/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7359 - acc: 0.6700     \n",
      "Epoch 50/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7407 - acc: 0.6710     \n",
      "Epoch 51/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7161 - acc: 0.6760     \n",
      "Epoch 52/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7347 - acc: 0.6750     \n",
      "Epoch 53/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7223 - acc: 0.6690     \n",
      "Epoch 54/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7061 - acc: 0.6910     \n",
      "Epoch 55/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7382 - acc: 0.6810     \n",
      "Epoch 56/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7353 - acc: 0.6750     \n",
      "Epoch 57/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7076 - acc: 0.6840     \n",
      "Epoch 58/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7048 - acc: 0.6760     \n",
      "Epoch 59/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7168 - acc: 0.6740     \n",
      "Epoch 60/60\n",
      "1000/1000 [==============================] - 2s - loss: 0.7428 - acc: 0.6630     \n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout,RepeatVector,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "\n",
    "embed_size = 50\n",
    "batch_size=32\n",
    "epochs=60\n",
    "\n",
    "story_model = Sequential()\n",
    "story_model.add(Embedding(vocab_size,embed_size,input_length=story_max))\n",
    "\n",
    "\n",
    "question_model = Sequential()\n",
    "question_model.add(Embedding(vocab_size,embed_size,input_length=question_max))\n",
    "question_model.add(LSTM(embed_size))\n",
    "question_model.add(RepeatVector(story_max)) #permet d'ajuster la taille du modèle afin de préparer un merge\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([story_model, question_model], mode='sum'))\n",
    "model.add(LSTM(embed_size))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X, Xq], Y, batch_size=batch_size, nb_epoch=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 992/1000 [============================>.] - ETA: 0s\n",
      "Perte = 0.712211671829\n",
      "Précision = 0.688\n"
     ]
    }
   ],
   "source": [
    "#Calcul de précision sur l'ensemble de test\n",
    "\n",
    "loss,acc = model.evaluate([X_test, Xq_test],Y_test, batch_size=batch_size)\n",
    "print \"\\nPerte = {}\".format(loss)\n",
    "print \"Précision = {}\".format(acc)\n",
    "\n",
    "def getClass(nd_matrix):\n",
    "    return np.argmax(nd_matrix,axis=1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
