{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur ce nb:\n",
    "\n",
    "- Parser/Vectorisation/Modèle\n",
    "- Evaluation du modèle\n",
    "\n",
    "Reste à perfectionner le modèle et/ou les autres paramètres. Utiliser un Memory Network pour après?\n",
    "\n",
    "# Projet: Facebook BaBi tasks\n",
    "Notebook de la semaine du 27/02\n",
    "Par Thierry Loesch et Bryce TIchit\n",
    "\n",
    "Sur ce notebook nous avons rajouté un parser afin de parser les données et nous avons fait la vectorisation de celles-ci. Un modèle a aussi été créé selon ce qui nous avions défini dans le notebook précédent. Nous avons ainsi pu commencer a faire des tests avec le modèle, et nous avons eu des résultats plutôt satisfaisant sur la première task, environ 70% de précision. Le réseau récurrent utilisé est LSTM sur ce notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parserBabi(data):\n",
    "    ret=list()\n",
    "    story=list()\n",
    "    \n",
    "    for phrase in data:\n",
    "        phrase = phrase.decode('utf-8').strip()\n",
    "        id_phrase,phrase = int(phrase[0]),phrase[1:]\n",
    "        \n",
    "        if id_phrase == 1: #Nouvelle story\n",
    "            story=list()\n",
    "            \n",
    "        if '\\t' in phrase: #Si tabulation alors il s'agit de la question ainsi que de la réponse\n",
    "            q, a, justif = phrase.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            \n",
    "            #la conditionelle permet de ne pas inclure les séparateurs\n",
    "            data_story = [x for x in story if x] #L'ensemble des élements qui nous permettra de raisonner sur le texte\n",
    "                                             \n",
    "            ret.append((data_story,q,a)) #Nos données d'apprentissages\n",
    "            story.append('')\n",
    "        \n",
    "        else: \n",
    "            #Alors la phrase est tout simplement un des élements de raisonnement et non une question\n",
    "            story.append(tokenize(phrase))\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def readAndParse(f):\n",
    "    data = parserBabi(f.readlines())\n",
    "    return [([substory for substories in story for substory in substories], q , a) for story,q,a in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des données\n",
    "\n",
    "On récupère les fichiers sur internet directement avec la fonction get_file, cette fonction a l'avantage de ne pas tout retelecharger si les données sont déjà sur la machines (dans ~/.keras/datasets). Puis on applique les fonctions du parser afin de récupérer les données dans la structure que l'on veut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import tarfile\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "   print(\"erreur pendant le telechargement\")\n",
    "    \n",
    "tar = tarfile.open(path)\n",
    "challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa19_path-finding_{}.txt'\n",
    "\n",
    "train = readAndParse(tar.extractfile(challenge.format('train')))\n",
    "test = readAndParse(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict(((w,i+1) for i,w in enumerate(vocab)))\n",
    "\n",
    "story_max = max((len(x) for x,_,_ in train+test))\n",
    "question_max = max((len(x) for _,x,_ in train+test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation\n",
    "\n",
    "Comme vu en cours on doit vectoriser nos données (les assimiler a des nombres et les mettre dans des matrices) afin de pouvoir entrainer notre réseau neuronal.\n",
    "\n",
    "La fonction pad_sequences plus bas permet de transformer notre liste de liste en matrice numpy en ajoutant des 0. pour complèter quand il n'y a pas de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mettons tout ça dans des matrices\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(data,word_idx,story_max,question_max):\n",
    "    \n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, question, reponse in train:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in question]\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[reponse]] = 1 \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    \n",
    "    return pad_sequences(X, maxlen=story_max), pad_sequences(Xq, maxlen=question_max), np.array(Y)\n",
    "\n",
    "X,Xq,Y = vectorize(train,word_idx,story_max,question_max)\n",
    "X_test,Xq_test,Y_test = vectorize(test,word_idx,story_max,question_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  7 18 21 20 11  1  6 23 21 20 14  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 18 21 20 11  1  6 23 21\n",
      "  20 14  1  5 23 10 21 20 14  1  8 18 21 20 13  1]\n",
      " [ 0  0  0  7 18 21 20 11  1  6 23 21 20 14  1  5 23 10 21 20 14  1  8 18\n",
      "  21 20 13  1  6 18 21 20 19  1  8 16 21 20 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  8 22 21 20 19  1  8 23 21 20 11  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8 22 21 20 19  1  8 23\n",
      "  21 20 11  1  7 23 21 20 12  1  5 18 21 20 14  1]\n",
      " [ 0  0  0  0  8 22 21 20 19  1  8 23 21 20 11  1  7 23 21 20 12  1  5 18\n",
      "  21 20 14  1  6 23 21 20 13  1  6 22 21 20 19  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "[[ 0  9 15  7  4]\n",
      " [ 0  9 15  5  4]\n",
      " [ 0  9 15  5  4]\n",
      " [ 2  9 15  5  4]\n",
      " [ 3  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 0  9 15  8  4]\n",
      " [ 2  9 15  6  4]\n",
      " [ 3  9 15  5  4]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:10]\n",
    "print Xq[0:10]\n",
    "print Y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle\n",
    "\n",
    "Ensuite nous crééons notre modèle keras, nous avions choisi donc de faire un modèle pour les story et pour les questions afin de pouvoir raisonner sur chacun d'eux différement puis de les combiner en un modèle, ainsi nous avons un seul modèle mais qui traite différement les story des questions.\n",
    "\n",
    "Schema modèle:\n",
    "\n",
    "                    story_model                              question_model\n",
    "                         |                                           |\n",
    "                      Embedding                                  Embedding\n",
    "                          |                                          |\n",
    "                       Dropout                                     Dropout\n",
    "                          |                                          /\n",
    "                        LSTM                                       /\n",
    "                          |                                       /\n",
    "                        RepeatVector                            /\n",
    "                                    \\                          /\n",
    "                                      \\_______________________/\n",
    "                                                   |\n",
    "                                                   |\n",
    "                                              Merge(mode=sum)\n",
    "                                                   |\n",
    "                                                 LSTM\n",
    "                                                   |\n",
    "                                                 Dropout\n",
    "                                                    |\n",
    "                                                 SoftMax\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Avec un batchsize de 32 un nombre de 50 epochs semble optimal.\n",
    "- Le fait d'avoir utilisé un batchsize petit a amelioré les résultats (32 optimal?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.9990 - acc: 0.1660     \n",
      "Epoch 2/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.8781 - acc: 0.2250     \n",
      "Epoch 3/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.6601 - acc: 0.3410     \n",
      "Epoch 4/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.5918 - acc: 0.3830     \n",
      "Epoch 5/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.5490 - acc: 0.4110     \n",
      "Epoch 6/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.5051 - acc: 0.4110     \n",
      "Epoch 7/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.4439 - acc: 0.4150     \n",
      "Epoch 8/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.3999 - acc: 0.4250     \n",
      "Epoch 9/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.3730 - acc: 0.4290     \n",
      "Epoch 10/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.3405 - acc: 0.4580     \n",
      "Epoch 11/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.3170 - acc: 0.4400     \n",
      "Epoch 12/80\n",
      "1000/1000 [==============================] - 3s - loss: 1.2989 - acc: 0.4400     \n",
      "Epoch 13/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.2767 - acc: 0.4650     \n",
      "Epoch 14/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.2493 - acc: 0.4510     \n",
      "Epoch 15/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.2252 - acc: 0.4730     \n",
      "Epoch 16/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.2234 - acc: 0.4730     \n",
      "Epoch 17/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.1911 - acc: 0.4680     \n",
      "Epoch 18/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.1946 - acc: 0.4970     \n",
      "Epoch 19/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.1688 - acc: 0.4840     \n",
      "Epoch 20/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.1174 - acc: 0.5010     \n",
      "Epoch 21/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0946 - acc: 0.5450     \n",
      "Epoch 22/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0997 - acc: 0.5400     \n",
      "Epoch 23/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0476 - acc: 0.5420     \n",
      "Epoch 24/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0546 - acc: 0.5290     \n",
      "Epoch 25/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0479 - acc: 0.5380     \n",
      "Epoch 26/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0132 - acc: 0.5580     \n",
      "Epoch 27/80\n",
      "1000/1000 [==============================] - 2s - loss: 1.0179 - acc: 0.5540     \n",
      "Epoch 28/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9866 - acc: 0.5630     \n",
      "Epoch 29/80\n",
      "1000/1000 [==============================] - 3s - loss: 1.0034 - acc: 0.5600     \n",
      "Epoch 30/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9490 - acc: 0.5840     \n",
      "Epoch 31/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9330 - acc: 0.5950     \n",
      "Epoch 32/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9223 - acc: 0.6020     \n",
      "Epoch 33/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9370 - acc: 0.5800     \n",
      "Epoch 34/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9085 - acc: 0.6050     \n",
      "Epoch 35/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8967 - acc: 0.6130     \n",
      "Epoch 36/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.9159 - acc: 0.6090     \n",
      "Epoch 37/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8584 - acc: 0.6460     \n",
      "Epoch 38/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8873 - acc: 0.6170     \n",
      "Epoch 39/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8622 - acc: 0.6320     \n",
      "Epoch 40/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8669 - acc: 0.6370     \n",
      "Epoch 41/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8368 - acc: 0.6330     \n",
      "Epoch 42/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8570 - acc: 0.6330     \n",
      "Epoch 43/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8383 - acc: 0.6250     \n",
      "Epoch 44/80\n",
      "1000/1000 [==============================] - 3s - loss: 0.8319 - acc: 0.6260     \n",
      "Epoch 45/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8035 - acc: 0.6400     \n",
      "Epoch 46/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8359 - acc: 0.6240     \n",
      "Epoch 47/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8191 - acc: 0.6340     \n",
      "Epoch 48/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8184 - acc: 0.6510     \n",
      "Epoch 49/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7856 - acc: 0.6590     \n",
      "Epoch 50/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8271 - acc: 0.6470     \n",
      "Epoch 51/80\n",
      "1000/1000 [==============================] - 3s - loss: 0.8162 - acc: 0.6440     \n",
      "Epoch 52/80\n",
      "1000/1000 [==============================] - 3s - loss: 0.7909 - acc: 0.6730     \n",
      "Epoch 53/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7931 - acc: 0.6530     \n",
      "Epoch 54/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7866 - acc: 0.6430     \n",
      "Epoch 55/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8118 - acc: 0.6330     \n",
      "Epoch 56/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7952 - acc: 0.6420     \n",
      "Epoch 57/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7851 - acc: 0.6500     \n",
      "Epoch 58/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7853 - acc: 0.6650     \n",
      "Epoch 59/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.8093 - acc: 0.6710     \n",
      "Epoch 60/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7934 - acc: 0.6420     \n",
      "Epoch 61/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7763 - acc: 0.6660     \n",
      "Epoch 62/80\n",
      "1000/1000 [==============================] - 3s - loss: 0.7568 - acc: 0.6680     \n",
      "Epoch 63/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7888 - acc: 0.6320     \n",
      "Epoch 64/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7528 - acc: 0.6710     \n",
      "Epoch 65/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7369 - acc: 0.6810     \n",
      "Epoch 66/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7791 - acc: 0.6680     \n",
      "Epoch 67/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7659 - acc: 0.6840     \n",
      "Epoch 68/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7605 - acc: 0.6600     \n",
      "Epoch 69/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7561 - acc: 0.6720     \n",
      "Epoch 70/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7656 - acc: 0.6590     \n",
      "Epoch 71/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7500 - acc: 0.6600     \n",
      "Epoch 72/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7802 - acc: 0.6640     \n",
      "Epoch 73/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7464 - acc: 0.6740     \n",
      "Epoch 74/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7426 - acc: 0.6570     \n",
      "Epoch 75/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7434 - acc: 0.6750     \n",
      "Epoch 76/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7482 - acc: 0.6580     \n",
      "Epoch 77/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7572 - acc: 0.6700     \n",
      "Epoch 78/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7593 - acc: 0.6700     \n",
      "Epoch 79/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7549 - acc: 0.6680     \n",
      "Epoch 80/80\n",
      "1000/1000 [==============================] - 2s - loss: 0.7587 - acc: 0.6710     \n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout,RepeatVector,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "\n",
    "embed_size = 50\n",
    "batch_size=32\n",
    "epochs=60\n",
    "\n",
    "story_model = Sequential()\n",
    "story_model.add(Embedding(vocab_size,embed_size,input_length=story_max))\n",
    "story_model.add(Dropout(0.3)) #La couche Dropout permet d'éviter sur-apprentissage\n",
    "\n",
    "question_model = Sequential()\n",
    "question_model.add(Embedding(vocab_size,embed_size,input_length=question_max))\n",
    "question_model.add(Dropout(0.3))\n",
    "question_model.add(LSTM(embed_size))\n",
    "question_model.add(RepeatVector(story_max)) #permet d'ajuster la taille du modèle afin de préparer un merge\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([story_model, question_model], mode='sum'))\n",
    "model.add(LSTM(embed_size))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X, Xq], Y, batch_size=batch_size, nb_epoch=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 992/1000 [============================>.] - ETA: 0s\n",
      "Perte = 0.697354410172\n",
      "Précision = 0.689\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate([X_test, Xq_test],Y_test, batch_size=batch_size)\n",
    "print \"\\nPerte = {}\".format(loss)\n",
    "print \"Précision = {}\".format(acc)\n",
    "\n",
    "def getClass(nd_matrix):\n",
    "    return np.argmax(nd_matrix,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
