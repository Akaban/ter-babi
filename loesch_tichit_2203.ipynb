{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Projet: Facebook BaBi tasks\n",
    "Notebook de la semaine du 22/03\n",
    "Par Thierry Loesch et Bryce Tichit\n",
    "\n",
    "Comme demandé par notre tuteur, nous allons désormais essayer d'adapter les travaux fait jusqu'ici sur un autre projet sensiblement identique.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "5.1  Notation automatique de réponses courtes à des questions\n",
    "\n",
    "Les données sont ici pour le téléchargement et leur description est dans cet article\n",
    "\n",
    "Lorsque l'on connait:\n",
    "\n",
    "    une question,\n",
    "    une réponse d'un étudiant,\n",
    "    la bonne réponse, \n",
    "\n",
    "comment prédire la note à donné à la réponse de l'étudiant. Il peut être aussi intéressant d'essayer d'apprendre à générer la réponse. \n",
    "\n",
    "--------------------------\n",
    "\n",
    "On cherchera en premier à prédire la note de l'étudiant, le réseau devra rendre un réel. Normaliser les notes (les mettre entre 0 et 1) et activation par Sigmoid (output entre 0 et 1) afin de prédire la note.\n",
    "\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Parser données\n",
    "- Vectorisation\n",
    "- Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.utils.data_utils import get_file\n",
    "import zipfile\n",
    "import string\n",
    "\n",
    "def tokenize(sent):\n",
    "    \n",
    "    def remPunctuation(sent):\n",
    "        return \"\".join(l for l in sent if l not in string.punctuation)\n",
    "        \n",
    "    return [x.strip() for x in re.split('(\\W+)?', remPunctuation(sent.lower())) if x.strip()]\n",
    "\n",
    "def parseData(questions,answers,student_answers=''):\n",
    "    \n",
    "    #questions_data = dict() #ce dictionnaire contiendra les tuples (question,answer) indexés par la clé \"numéro de question\"\n",
    "    \n",
    "    questions_data = dict()\n",
    "    \n",
    "    for line in questions.split('\\n'): #une ligne = une question\n",
    "        line = line.decode('utf-8').strip()\n",
    "        try:\n",
    "            index,question = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        questions_data[index] = [question.replace('<STOP>',''),None]\n",
    "    \n",
    "    for line in answers.split('\\n'):\n",
    "        line = line.decode('utf-8').strip()\n",
    "        \n",
    "        try:\n",
    "            index,answer = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        questions_data[index][1] = answer.replace('<STOP>','')\n",
    "        \n",
    "    questions_data = {k: tuple(map(tokenize,v)) for k, v in questions_data.items()}\n",
    "    \n",
    "    stud_ans = dict()\n",
    "    \n",
    "    for line in student_answers.split('\\n'):\n",
    "        line = line.decode('utf-8')\n",
    "        try:\n",
    "            index,ans = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        answers=ans.split('<STOP>')\n",
    "        answers=[a for a in answers if a]\n",
    "            \n",
    "        if index in stud_ans:\n",
    "            stud_ans[index] += map(tokenize,answers)\n",
    "        else:\n",
    "            stud_ans[index] = map(tokenize,answers)\n",
    "    \n",
    "    return questions_data,stud_ans\n",
    "        \n",
    "    \n",
    "def parseIntoExamples(qd,sa):\n",
    "    \n",
    "    ret = list()\n",
    "    \n",
    "    for index,q_a in qd.items():\n",
    "        for stud_ans in sa[index]:\n",
    "            ret.append((q_a[0],q_a[1],stud_ans))\n",
    "    return ret\n",
    "                       \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    path = get_file('shortAnswerGrading-v-2-0.zip',origin='http://web.eecs.umich.edu/~mihalcea/downloads/ShortAnswerGrading_v2.0.zip')\n",
    "except:\n",
    "    print \"error while downloading file\"\n",
    "    \n",
    "    \n",
    "archive = zipfile.ZipFile(path,'r')\n",
    "    \n",
    "questions = archive.read('data/sent/questions') \n",
    "answers = archive.read('data/sent/answers')\n",
    "student_answers = archive.read('data/sent/all') \n",
    "\n",
    "dic1,dic2 = parseData(questions,answers,student_answers)\n",
    "\n",
    "\n",
    "data = parseIntoExamples(dic1,dic2)\n",
    "\n",
    "#print data[0]\n",
    "\n",
    "vocab = set()\n",
    "for el in data:\n",
    "    for x in el:\n",
    "        vocab.update(x)\n",
    "vocab=sorted(list(vocab))\n",
    "\n",
    "vocab[0:300]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_idx = dict((c,i+1) for i,c in enumerate(vocab))\n",
    "question_maxsize = max((len(x) for x,_,_ in data))\n",
    "answer_maxsize = max((max(len(x),len(y)) for _,x,y in data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire:\n",
    "- Nettoyer le vocabulaire pour diminuer sa taille, synonymes, lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mettons tout ça dans des matrices\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(data,wordidx,qmaxlen,amaxlen):\n",
    "    X,Xq,Y = list(),list(),list()\n",
    "    \n",
    "    lookup = lambda m : wordidx[m]\n",
    "    \n",
    "    for question, answer, student_answer in data:\n",
    "        X.append(map(lookup,question))\n",
    "        Xs.append(map(lookup,student_answer))\n",
    "        Y.append(map(lookup,))\n",
    "        Yline = np.zeros(vocab_size)\n",
    "        Yline[wordidx[reponse]] = 1\n",
    "        Y.append(Yline)\n",
    "        \n",
    "    return pad_sequences(X, maxlen=story_max), pad_sequences(Xq, maxlen=question_max), np.array(Y)\n",
    "        \n",
    "    \n",
    "X,Xq,Y = vectorize(train,word_idx,story_max,question_max)\n",
    "X_test,Xq_test,Y_test = vectorize(test,word_idx,story_max,question_max)\n",
    "X_val,Xq_val,Y_val = vectorize(validation,word_idx,story_max,question_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
