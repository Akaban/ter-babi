{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Projet: Facebook BaBi tasks\n",
    "Notebook de la semaine du 22/03\n",
    "Par Thierry Loesch et Bryce Tichit\n",
    "\n",
    "Dernier notebook\n",
    "\n",
    "Comme demandé par notre tuteur, nous allons désormais essayer d'adapter les travaux fait jusqu'ici sur un autre projet sensiblement identique.\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "5.1  Notation automatique de réponses courtes à des questions\n",
    "\n",
    "Les données sont ici pour le téléchargement et leur description est dans cet article\n",
    "\n",
    "Lorsque l'on connait:\n",
    "\n",
    "    une question,\n",
    "    une réponse d'un étudiant,\n",
    "    la bonne réponse, \n",
    "\n",
    "comment prédire la note à donné à la réponse de l'étudiant. Il peut être aussi intéressant d'essayer d'apprendre à générer la réponse. \n",
    "\n",
    "--------------------------\n",
    "\n",
    "On cherchera en premier à prédire la note de l'étudiant, le réseau devra rendre un réel. Normaliser les notes (les mettre entre 0 et 1) et activation par Sigmoid (output entre 0 et 1) afin de prédire la note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.utils.data_utils import get_file\n",
    "import zipfile\n",
    "import string\n",
    "\n",
    "def tokenize(sent):\n",
    "    \n",
    "    def remPunctuation(sent):\n",
    "        return \"\".join(l for l in sent if l not in string.punctuation)\n",
    "        \n",
    "    return [x.strip() for x in re.split('(\\W+)?', remPunctuation(sent.lower())) if x.strip()]\n",
    "\n",
    "def parseData(questions,answers,student_answers=''):\n",
    "    \n",
    "    #questions_data = dict() #ce dictionnaire contiendra les tuples (question,answer) indexés par la clé \"numéro de question\"\n",
    "    \n",
    "    questions_data = dict()\n",
    "    \n",
    "    for line in questions.split('\\n'): #une ligne = une question\n",
    "        line = line.decode('utf-8').strip()\n",
    "        try:\n",
    "            index,question = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        questions_data[index] = [question.replace('<STOP>',''),None]\n",
    "    \n",
    "    for line in answers.split('\\n'):\n",
    "        line = line.decode('utf-8').strip()\n",
    "        \n",
    "        try:\n",
    "            index,answer = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        questions_data[index][1] = answer.replace('<STOP>','')\n",
    "        \n",
    "    questions_data = {k: tuple(map(tokenize,v)) for k, v in questions_data.items()}\n",
    "    \n",
    "    stud_ans = dict()\n",
    "    \n",
    "    for line in student_answers.split('\\n'):\n",
    "        line = line.decode('utf-8')\n",
    "        try:\n",
    "            index,ans = line.split(' ',1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        answers=ans.split('<STOP>')\n",
    "        answers=[a for a in answers if a]\n",
    "            \n",
    "        if index in stud_ans:\n",
    "            stud_ans[index] += map(tokenize,answers)\n",
    "        else:\n",
    "            stud_ans[index] = map(tokenize,answers)\n",
    "    \n",
    "    return questions_data,stud_ans\n",
    "        \n",
    "    \n",
    "def parseIntoExamples(qd,sa):\n",
    "    \n",
    "    ret = list()\n",
    "    \n",
    "    for index,q_a in qd.items():\n",
    "        scores = [k for k in archive.read('data/scores/'+str(index+'/ave')).split('\\n') if k]\n",
    "        for stud_ans,score in zip(sa[index],scores):\n",
    "            ret.append((q_a[0],q_a[1],stud_ans,float(score)/5.))\n",
    "    return ret\n",
    "                       \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0',\n",
       " u'000000',\n",
       " u'012',\n",
       " u'0x',\n",
       " u'1',\n",
       " u'10',\n",
       " u'115',\n",
       " u'12345',\n",
       " u'123456789',\n",
       " u'124',\n",
       " u'154',\n",
       " u'18',\n",
       " u'1each',\n",
       " u'1long',\n",
       " u'1st',\n",
       " u'2',\n",
       " u'20',\n",
       " u'256',\n",
       " u'2nd',\n",
       " u'2the',\n",
       " u'3',\n",
       " u'35',\n",
       " u'3n',\n",
       " u'4',\n",
       " u'40',\n",
       " u'400',\n",
       " u'5',\n",
       " u'5555',\n",
       " u'5657',\n",
       " u'6',\n",
       " u'68',\n",
       " u'7',\n",
       " u'72',\n",
       " u'8',\n",
       " u'80',\n",
       " u'88123',\n",
       " u'8th',\n",
       " u'a',\n",
       " u'ability',\n",
       " u'able',\n",
       " u'about',\n",
       " u'above',\n",
       " u'abstract',\n",
       " u'abstraction',\n",
       " u'accepted',\n",
       " u'accepts',\n",
       " u'accesed',\n",
       " u'access',\n",
       " u'accessed',\n",
       " u'accessible',\n",
       " u'accessing',\n",
       " u'accessmodifier',\n",
       " u'accessspecifications',\n",
       " u'accessspecifiers',\n",
       " u'accomplish',\n",
       " u'according',\n",
       " u'accordingly',\n",
       " u'account',\n",
       " u'accurate',\n",
       " u'achieved',\n",
       " u'acknowledged',\n",
       " u'acordingly',\n",
       " u'across',\n",
       " u'act',\n",
       " u'acted',\n",
       " u'actions',\n",
       " u'activity',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'ad',\n",
       " u'add',\n",
       " u'added',\n",
       " u'adding',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'address',\n",
       " u'addressed',\n",
       " u'addresses',\n",
       " u'adds',\n",
       " u'adjacent',\n",
       " u'adjust',\n",
       " u'adjusted',\n",
       " u'adt',\n",
       " u'advance',\n",
       " u'advantage',\n",
       " u'advantages',\n",
       " u'advises',\n",
       " u'affect',\n",
       " u'affected',\n",
       " u'affecting',\n",
       " u'affects',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'agian',\n",
       " u'algorithm',\n",
       " u'algorithms',\n",
       " u'algorithyms',\n",
       " u'alias',\n",
       " u'all',\n",
       " u'allocate',\n",
       " u'allocated',\n",
       " u'allocation',\n",
       " u'allot',\n",
       " u'allow',\n",
       " u'allowed',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'almost',\n",
       " u'along',\n",
       " u'already',\n",
       " u'also',\n",
       " u'alter',\n",
       " u'altered',\n",
       " u'alternate',\n",
       " u'alternative',\n",
       " u'always',\n",
       " u'ammout',\n",
       " u'among',\n",
       " u'amonts',\n",
       " u'amount',\n",
       " u'amounts',\n",
       " u'amout',\n",
       " u'ampersand',\n",
       " u'an',\n",
       " u'ancestor',\n",
       " u'ancestors',\n",
       " u'and',\n",
       " u'another',\n",
       " u'answer',\n",
       " u'answered',\n",
       " u'answers',\n",
       " u'anther',\n",
       " u'any',\n",
       " u'anything',\n",
       " u'anytime',\n",
       " u'anywhere',\n",
       " u'appear',\n",
       " u'application',\n",
       " u'applications',\n",
       " u'applied',\n",
       " u'approach',\n",
       " u'appropriate',\n",
       " u'aptr',\n",
       " u'arangment',\n",
       " u'arbitrary',\n",
       " u'architecture',\n",
       " u'are',\n",
       " u'area',\n",
       " u'argument',\n",
       " u'arguments',\n",
       " u'around',\n",
       " u'arranged',\n",
       " u'array',\n",
       " u'arraybased',\n",
       " u'arrayname',\n",
       " u'arrayptr',\n",
       " u'arrays',\n",
       " u'arraysize',\n",
       " u'arriving',\n",
       " u'as',\n",
       " u'assessment',\n",
       " u'assigned',\n",
       " u'assigning',\n",
       " u'assigns',\n",
       " u'associated',\n",
       " u'assume',\n",
       " u'assuming',\n",
       " u'at',\n",
       " u'atleast',\n",
       " u'atributes',\n",
       " u'attach',\n",
       " u'attached',\n",
       " u'attaching',\n",
       " u'attatched',\n",
       " u'attempt',\n",
       " u'attribute',\n",
       " u'attributes',\n",
       " u'attrubutes',\n",
       " u'autocreate',\n",
       " u'automatic',\n",
       " u'automatically',\n",
       " u'available',\n",
       " u'avariable',\n",
       " u'average',\n",
       " u'avl',\n",
       " u'avoid',\n",
       " u'avoiding',\n",
       " u'avoids',\n",
       " u'away',\n",
       " u'awesome',\n",
       " u'b',\n",
       " u'back',\n",
       " u'backtrack',\n",
       " u'backup',\n",
       " u'backward',\n",
       " u'backwards',\n",
       " u'badly',\n",
       " u'balanced',\n",
       " u'bank',\n",
       " u'base',\n",
       " u'based',\n",
       " u'bases',\n",
       " u'basic',\n",
       " u'basically',\n",
       " u'basics',\n",
       " u'basis',\n",
       " u'be',\n",
       " u'because',\n",
       " u'become',\n",
       " u'becomes',\n",
       " u'been',\n",
       " u'before',\n",
       " u'begin',\n",
       " u'beging',\n",
       " u'begining',\n",
       " u'beginning',\n",
       " u'begins',\n",
       " u'behavior',\n",
       " u'behaviors',\n",
       " u'behaviour',\n",
       " u'behind',\n",
       " u'being',\n",
       " u'believe',\n",
       " u'belong',\n",
       " u'belonging',\n",
       " u'below',\n",
       " u'benefit',\n",
       " u'besides',\n",
       " u'best',\n",
       " u'bestcase',\n",
       " u'better',\n",
       " u'between',\n",
       " u'beyond',\n",
       " u'bidimensional',\n",
       " u'big',\n",
       " u'bigger',\n",
       " u'biggest',\n",
       " u'bigo',\n",
       " u'bigoh',\n",
       " u'bin',\n",
       " u'binary',\n",
       " u'bit',\n",
       " u'bits',\n",
       " u'blah',\n",
       " u'block',\n",
       " u'blocks',\n",
       " u'bodies',\n",
       " u'body',\n",
       " u'both',\n",
       " u'bothe',\n",
       " u'bottom',\n",
       " u'bounds',\n",
       " u'bpointer',\n",
       " u'bptr',\n",
       " u'brackets',\n",
       " u'brain',\n",
       " u'branch',\n",
       " u'branched',\n",
       " u'branches',\n",
       " u'branching',\n",
       " u'break',\n",
       " u'breaking',\n",
       " u'breaks',\n",
       " u'brief',\n",
       " u'briefly',\n",
       " u'brings',\n",
       " u'broken',\n",
       " u'bugs',\n",
       " u'build',\n",
       " u'building',\n",
       " u'built',\n",
       " u'burn',\n",
       " u'but',\n",
       " u'by',\n",
       " u'bydimensional',\n",
       " u'byte',\n",
       " u'bytes',\n",
       " u'c',\n",
       " u'calculate',\n",
       " u'calculations',\n",
       " u'call',\n",
       " u'called',\n",
       " u'caller',\n",
       " u'calling',\n",
       " u'calls',\n",
       " u'can',\n",
       " u'candidate',\n",
       " u'cannot',\n",
       " u'cant',\n",
       " u'capable',\n",
       " u'carry',\n",
       " u'case',\n",
       " u'case1',\n",
       " u'casen',\n",
       " u'cases',\n",
       " u'cast',\n",
       " u'cause',\n",
       " u'causes',\n",
       " u'causing']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    path = get_file('shortAnswerGrading-v-2-0.zip',origin='http://web.eecs.umich.edu/~mihalcea/downloads/ShortAnswerGrading_v2.0.zip')\n",
    "except:\n",
    "    print \"error while downloading file\"\n",
    "    \n",
    "    \n",
    "archive = zipfile.ZipFile(path,'r')\n",
    "    \n",
    "questions = archive.read('data/sent/questions') \n",
    "answers = archive.read('data/sent/answers')\n",
    "student_answers = archive.read('data/sent/all') \n",
    "\n",
    "dic1,dic2 = parseData(questions,answers,student_answers)\n",
    "\n",
    "\n",
    "data = parseIntoExamples(dic1,dic2)\n",
    "\n",
    "#print data[0]\n",
    "\n",
    "vocab = set()\n",
    "for (a,b,c,_) in data:\n",
    "    for x in [a,b,c]:\n",
    "        vocab.update(x)\n",
    "vocab=sorted(list(vocab))\n",
    "\n",
    "print len(vocab)\n",
    "\n",
    "vocab_size = len(vocab) +1\n",
    "word_idx = dict((c,i+1) for i,c in enumerate(vocab))\n",
    "question_maxsize = max((len(x) for x,_,_,_ in data))\n",
    "answer_stud_maxsize = max(len(x) for _,_,x,_ in data)\n",
    "answer_maxsize = max((len(x) for _,x,_,_ in data))\n",
    "\n",
    "vocab[0:300]\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire:\n",
    "- Nettoyer le vocabulaire pour diminuer sa taille, synonymes, lowercase\n",
    "\n",
    "Pour cela on peut utiliser NTLK et les Stemmer, afin de réduire significativement la taille du vocabulaire, également voir ce  qu'on peut faire pour les nombres. Les associer à un unique mot?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mettons tout ça dans des matrices\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(data,wordidx,qmaxlen,amaxlen):\n",
    "    X,Xs,Y = list(),list(),list()\n",
    "    \n",
    "    lookup = lambda m : wordidx[m]\n",
    "    \n",
    "    for question, _ , student_answer, score in data:\n",
    "        X.append(map(lookup,question))\n",
    "        Xs.append(map(lookup,student_answer))\n",
    "        Y.append(score)\n",
    "        \n",
    "    return pad_sequences(X, maxlen=qmaxlen), pad_sequences(Xs, maxlen=amaxlen), np.array(Y)\n",
    "        \n",
    "    \n",
    "X,Xq,Y = vectorize(data,word_idx,question_maxsize,answer_stud_maxsize)\n",
    "#X_test,Xq_test,Y_test = vectorize(test,word_idx,story_max,question_max)\n",
    "#X_val,Xq_val,Y_val = vectorize(validation,word_idx,story_max,question_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 2176 1040   38  832 1464]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0   38 1464 2016 1468 2050 2018 1230   76 1333 2018  217 1411\n",
      "  1333   38  835 1509 1267 2018 1464 1468 2050 2018  832 1860 1048  287\n",
      "   208  283 2050  722]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0   38  832 1464 1040   38 1464 2016  423 2018   76 1333\n",
      "  2018  832  949 1230]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  832 1466  148 1466  922  620 2136 2184 1462 2050 2018\n",
      "    76 1333   38  832]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0   38 1464 2050   38  832 1040 2018   76 2181 2018  337\n",
      "   811 2018  832 1695]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0 2029  287  208 1418 2050  835 1714  826  835 1933  949  158  128\n",
      "   163 2050 1380 1466]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0 1048  423 2018   76 1333\n",
      "  2018  832  949 1230]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 2178 1048 1040  535   38  832 1464  286 1064  832 2197\n",
      "  2222 1368 1259  151]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0   38\n",
      "  1464 2050   38  832]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0   38 1464\n",
      "  2050   38  832 1065  423 2018   76 1333 2018  832  128  287  208 2120\n",
      "  2050  282 2016  832]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  832 1466  148 1466  922  620 2136 2184 1462 2050 2018\n",
      "    76 1333   38  832]]\n",
      "[ 1.   1.   1.   1.   1.   0.7  0.7  1.   1.   1. ]\n"
     ]
    }
   ],
   "source": [
    "print X[0:10]\n",
    "print Xq[0:10]\n",
    "print Y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele\n",
    "\n",
    "Il s'agit du même modèle que pour les babi-tasks mais legèrement modifié, voir ce qu'on peut tenter comme modèle (question? answer? stud_answer?) mais à priori garder la même architecture.\n",
    "\n",
    "Actuellement les performances sont faibles à cause de la taille du vocabulaire (embed_size trop petit pour sa taille) il faut augmenter l'embed_size de manière raisonnable mais aussi diminuer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_29 (Embedding)         (None, 35, 100)       222400                                       \n",
      "____________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)         (None, 74, 100)       222400                                       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                   (None, 100)           80400                                        \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_14 (RepeatVector)   (None, 35, 100)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                   (None, 100)           120400      merge_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             101         lstm_24[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 1)             0           dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 645701\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge,RepeatVector,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "\n",
    "\n",
    "embed_size = 100\n",
    "batch_size=256\n",
    "epochs=10\n",
    "\n",
    "question_model = Sequential()\n",
    "question_model.add(Embedding(vocab_size,embed_size,input_length=question_maxsize))\n",
    "\n",
    "ans_model = Sequential()\n",
    "ans_model.add(Embedding(vocab_size,embed_size,input_length=answer_stud_maxsize))\n",
    "ans_model.add(LSTM(embed_size))\n",
    "ans_model.add(RepeatVector(question_maxsize)) #permet d'ajuster la taille du modèle afin de préparer un merge\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([question_model, ans_model], mode='concat'))\n",
    "model.add(LSTM(embed_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.01),loss='mse',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#history = model.fit([X, Xq], Y, batch_size=batch_size, nb_epoch=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
