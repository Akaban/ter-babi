{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Facebook BaBi tasks\n",
    "Notebook de la semaine du 30/01\n",
    "Par Thierry Loesch et Bryce TIchit\n",
    "\n",
    "Nous avons choisi le Facebook BaBi tasks comme projet pour ce TER, le but de celui-ci étant de pouvoir raisonner sur un texte qui raconte une histoire et ainsi de pouvoir répondre à une question concernant cette même histoire.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "1 Mary moved to the bathroom.\n",
    "\n",
    "2 John went to the hallway.\n",
    "\n",
    "3 Where is Mary?        bathroom        1\n",
    "\n",
    "\n",
    "Dans cet échantillon d'apprentissage, nous avons un ensemble de phrases ordonné constituant une histoire, puis une question (where is mary) à laquelle on répond par \"bathroom\" car il s'agit de l'endroit où se trouve mary. Chaque phrase a son id dans l'histoire ainsi que la question, l'id qui se trouve après la réponse correspond à la phrase qui justifie cette réponse.\n",
    "\n",
    "Nous commencerons par examiner les données, les caractériser, puis nous proposerons des solutions possibles. Les BaBi tasks sont au nombre de 20, avec un palier de difficulté supplémentaire par task. En effet, plus on avance, plus les réponses sont complexes à obtenir et nécessitent de raisonner sur plusieurs phrases en même temps. Pour ce notebook nous nous cantonnerons à la première task qui ne nécessite qu'une phrase par question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7afb6a811812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1337\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# for reproducibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Programmes\\Anaconda2\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Programmes\\Anaconda2\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Programmes\\Anaconda2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Le code suivant permet de récupérer et parser les données.\n",
    "Source: https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py\n",
    "Note: Nous écrirons notre propre parser par la suite, nous avons repris celui donné en exemple afin de gagner\n",
    "du temps pour ce notebook\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryce/anaconda3/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Récupération des données et parsing\n",
    "Le code provient de https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "tar = tarfile.open(path)\n",
    "challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "#challenge = 'tasks_1-20_v1-2/en/qa19_path-finding_{}.txt'\n",
    "\n",
    "train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire: 21\n",
      "Nombre de caractères: 29\n",
      "Nombre de stories (apprentissage) : 1000\n",
      "Nombre de stories (test) : 1000\n"
     ]
    }
   ],
   "source": [
    "def uniq(l):\n",
    "    ret = set()\n",
    "    for el in l:\n",
    "        ret.add(el)\n",
    "    return list(ret)\n",
    "\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "caracteres = uniq(flatten(map(list,vocab)))\n",
    "sentences_from_stories = list(map(lambda d : d[0],train+test))\n",
    "\n",
    "distribution = dict()\n",
    "for s in sentences_from_stories:\n",
    "    len_s = len(s)\n",
    "    if len_s in distribution.keys():\n",
    "        distribution[len_s] +=1\n",
    "    else:\n",
    "        distribution[len_s] = 1\n",
    "        \n",
    "for i in range(100):\n",
    "    if i not in distribution.keys():\n",
    "        distribution[i]=0\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "print(\"Taille du vocabulaire: {}\".format(len(vocab)))\n",
    "print(\"Nombre de caractères: {}\".format(len(caracteres)))\n",
    "print(\"Nombre de stories (apprentissage) : {}\".format(len(test)))\n",
    "print(\"Nombre de stories (test) : {}\".format(len(test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la taille du vocabulaire est relativement réduite, en effet les exemples sont assez repétitifs et utilisent peu de mots. Nous n'avons donc pas besoin de le reduire. Cela vaut également pour le nombre de caractères. Notons que nous ne regarderons que les données de la première task pour l'instant mais globalement le vocabulaire et le nombre de caractères restent assez petits sur l'ensemble des tasks.\n",
    "\n",
    "Nos exemples sont répartis par \"story\", c'est à dire que nous avons une histoire (un ensemble de phrases) qui décrit certaines choses, une question sur cette même histoire ainsi que la réponse associée. Nous avons ainsi 1000 stories pour l'ensemble d'apprentissage et le même nombre pour l'ensemble de test. Si l'on préfère compter l'ensemble des phrases pour chaque story nous en avons 74297.\n",
    "\n",
    "Observons désormais la distribution de la longueur des stories en mots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0f31a60668b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distribution' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x,y=[],[]\n",
    "els = sorted(distribution.items(),key=(lambda k:k[0]))\n",
    "\n",
    "\n",
    "for ex,ey in els:\n",
    "    x.append(ex)\n",
    "    y.append(ey)\n",
    "\n",
    "x,y = np.asarray(x),np.asarray(y)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "plt.barplot(x,y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons un grand nombre de petites stories (12-13-14 mots) alors que les stories plus complexes sont moins nombreuses. Plus on augmente de nombre de mots par story, moins il y a de story disponible.\n",
    "\n",
    "Après avoir analysé ces données, il en ressort que la solution première semble être d'utiliser un réseau neuronal récurrent. En effet nous allons devoir garder un historique des phrases que nous allons lire afin de pouvoir répondre à la question par la suite, ce qui correspond à un réseau neuronal récurrent. Tout le problème est que nous avons deux données à traiter. Par exemple: des histoires et des questions (sachant que la réponse sera le résultat attendu pour les deux). Une idée est de d'abord séparer le problème en deux en créant un modèle pour les stories et un modèle pour les questions, on pourra ensuite effectuer un traitement différent sur ces derniers et décider de la manière dont nous fusionnerons ces deux réseaux. Pour ce faire nous pouvons utiliser la couche Merge de keras : https://keras.io/layers/core/#merge ce qui nous donnera un unique modèle à base de réseaux récurrents.\n",
    "\n",
    "Une autre idée serait d'utiliser des \"memory networks\", ces réseaux récemment décrit dans cet article (https://arxiv.org/abs/1410.3916) semblent adaptés à la résolution de ce problème et en particulier afin de raisonner sur différents élements. Par ailleurs ils ont été créés spécialement pour résoudre ce problème.\n",
    "\n",
    "Il nous reste donc à déterminer l'architecture des deux modèles que nous allons fusionner afin d'avoir un premier jet.\n",
    "\n",
    "Nous établissons pour la suite une liste des tâches à faire afin de traiter ce problème (non exhaustive):\n",
    "- Ecrire un parser pour les données bAbI\n",
    "- Vectoriser nos exemples (avec un embdedding)\n",
    "- Déterminer l'architecture optimale du modèle Story\n",
    "- Déterminer l'architecture optimale du modèle Question\n",
    "- Déterminer l'architecture optimale du modèle final -> En testant différentes configurations possibles\n",
    "\n",
    "Par la suite nous pourrons essayer de voir les résultats que nous pourrons avoir avec un memory network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
